import requests
import pandas as pd
import json
from pathlib import Path
from typing import Any, Dict, List
import numpy as np


API_URL = "https://esggenplus.twse.com.tw/api/api/mopsEsg/singleCompanyData"
USE_CHINESE_HEADERS = False  # True=è¼¸å‡ºä¸­æ–‡æ¬„ä½, False=è¼¸å‡ºä»£ç¢¼


COMPANIES = {
    "2330": "å°ç©é›»",
    "2317": "é´»æµ·",
    "2454": "è¯ç™¼ç§‘",
    "2881": "å¯Œé‚¦é‡‘",
    "2412": "ä¸­è¯é›»",
    "2382": "å»£é”",
    "2308": "å°é”é›»",
    "2882": "åœ‹æ³°é‡‘",
    "2891": "ä¸­ä¿¡é‡‘",
    "3711": "æ—¥æœˆå…‰æŠ•æ§",
}

YEARS = [2024, 2023]  # ä½ è¦è¼¸å‡ºçš„å¹´åº¦

OUTDIR = Path("./esg_scraper_all_2023_2024")
RAW_DIR = OUTDIR / "raw"
CSV_DIR = OUTDIR / "csv"
RAW_DIR.mkdir(parents=True, exist_ok=True)
CSV_DIR.mkdir(parents=True, exist_ok=True)

HEADERS = {
    "Content-Type": "application/json;charset=UTF-8",
    "Origin": "https://esggenplus.twse.com.tw",
    "Referer": "https://esggenplus.twse.com.tw/",
    "User-Agent": "Mozilla/5.0",
}


def make_lookup_records(df: pd.DataFrame) -> list[dict]:
    def to_native(x):
        if pd.isna(x):
            return None
        if isinstance(x, (np.integer,)):
            return int(x)
        if isinstance(x, (np.floating,)):
            return float(x)
        return x

    records = []
    for (code, name, year), g in df.groupby(["å…¬å¸ä»£ç¢¼", "å…¬å¸åç¨±", "å¹´åº¦"]):
        row = {
            "å…¬å¸ä»£ç¢¼": str(code),
            "å…¬å¸åç¨±": str(name),
            "å¹´åº¦": int(year) if not isinstance(year, str) else year,  # è½‰æˆåŸç”Ÿ int
        }
        for _, r in g.iterrows():
            key = f"{r['é¡åˆ¥']}/{r['æŒ‡æ¨™åç¨±']}/{r['å€æ®µ']}/{r['æ¬„ä½åç¨±']}"
            row[key] = to_native(r["æ•¸å€¼"])  # æ•¸å€¼è½‰æˆåŸç”Ÿ
        records.append(row)
    return records


def call_api(code: str, year: int) -> Dict[str, Any]:
    payload = {
        "companyCode": code,
        "yearList": [year],
        "companyName": None,
        "year": year,
    }
    r = requests.post(API_URL, headers=HEADERS, json=payload, timeout=30)
    r.raise_for_status()
    return r.json()


def save_raw(code: str, year: int, data: Any):
    path = RAW_DIR / f"{code}_{year}_raw.json"
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def flatten_payload(code: str, name: str, year: int, payload: Dict[str, Any]) -> pd.DataFrame:
    rows = []
    data_list = payload.get("data", [])
    if not isinstance(data_list, list) or not data_list:
        return pd.DataFrame()

    root = data_list[0]
    models = root.get("treeModels", [])

    for block in models:
        category = block.get("categoryString") or block.get("category")

        # ğŸ”¹ åªè¦ ç¤¾æœƒ / æ²»ç†
        if category not in ("ç’°å¢ƒ", "ç¤¾æœƒ", "æ²»ç†"):
            continue

        for it in block.get("items", []):
            declare_item = (
                it.get("declareItemShowName")
                or it.get("declareItemName")
                or it.get("item")
            )
            for sec in it.get("sections", []):
                section_name = sec.get("showName") or sec.get("name")
                for ctrl in sec.get("controls", []):
                    def _clean_value(val):
                        if val is None:
                            return None
                        if isinstance(val, str):
                            v = val.strip()
                            # ğŸ”¹ ç™¾åˆ†æ¯”è™•ç†ï¼ˆè½‰æˆæ¯”ä¾‹ 0~1ï¼‰
                            if v.endswith("%"):
                                try:
                                    return float(v.replace("%", "").replace(",", "").strip()) / 100
                                except ValueError:
                                    return val  # è½‰æ›å¤±æ•—å°±ä¿ç•™å­—ä¸²
                            # ğŸ”¹ ä¸€èˆ¬æ•¸å­—ï¼ˆå«åƒåˆ†ä½ï¼‰
                            try:
                                return float(v.replace(",", ""))
                            except ValueError:
                                return val
                        return val  # å·²ç¶“æ˜¯æ•¸å­—å°±ç›´æ¥å›å‚³

                    rows.append({
                        "companyCode": code,
                        "companyName": name,
                        "year": year,
                        "category": category,
                        "declareItemName": declare_item,
                        "section": section_name,
                        "controlTitle": ctrl.get("showTitle") or ctrl.get("title"),
                        "code": ctrl.get("code"),
                        "value": _clean_value(ctrl.get("value")),  # âœ… æ”¹é€™è£¡
                        "ctrType": ctrl.get("ctrType"),
                    })

    df = pd.DataFrame(rows)

    # æ”¹æ¬„ä½åç¨±æˆä¸­æ–‡
    df = df.rename(columns={
        "companyCode": "å…¬å¸ä»£ç¢¼",
        "companyName": "å…¬å¸åç¨±",
        "year": "å¹´åº¦",
        "category": "é¡åˆ¥",
        "declareItemName": "æŒ‡æ¨™åç¨±",
        "section": "å€æ®µ",
        "controlTitle": "æ¬„ä½åç¨±",
        "code": "æŒ‡æ¨™ä»£ç¢¼",
        "value": "æ•¸å€¼",
        "ctrType": "æ•¸æ“šå‹æ…‹",
    })

    return df


def main():
    all_dfs: List[pd.DataFrame] = []

    # å…ˆæŠŠæ‰€æœ‰ å…¬å¸Ã—å¹´åº¦ é€ä¸€æŠ“ä¸‹ä¾† â†’ ç´¯ç©åˆ° all_dfs
    for code, name in COMPANIES.items():
        for y in YEARS:
            try:
                payload = call_api(code, y)
                save_raw(code, y, payload)  # å­˜åŸå§‹ JSONï¼ˆå‰ç«¯ä¹Ÿå¯ç›´æ¥ç”¨ï¼‰
                df = flatten_payload(code, name, y, payload)
                if not df.empty:
                    # å–®å®¶å…¬å¸Ã—å¹´åº¦çš„é•·è¡¨
                    out_path = CSV_DIR / f"{code}_{y}_long.csv"
                    df.to_csv(out_path, index=False, encoding="utf-8-sig")
                    print(f"âœ” {name}({code}) å¹´åº¦ {y} â†’ {out_path}")
                    all_dfs.append(df)
                else:
                    print(f"âš  {name}({code}) å¹´åº¦ {y} æ²’æœ‰è³‡æ–™")
            except Exception as e:
                print(f"âœ— {name}({code}) å¹´åº¦ {y} å¤±æ•—: {e}")

    # === è¿´åœˆå…¨éƒ¨è·‘å®Œï¼Œå†åšåˆä½µè¼¸å‡º ===
    if not all_dfs:
        print("âœ— æ²’æœ‰ä»»ä½•è³‡æ–™å¯åˆä½µï¼Œè«‹æª¢æŸ¥ raw/*.json")
        return

    merged = pd.concat(all_dfs, ignore_index=True)
    merged.to_csv(OUTDIR / "all_companies_long.csv", index=False, encoding="utf-8-sig")
    print(f"ğŸ“„ åˆä½µè¼¸å‡ºï¼š{(OUTDIR / 'all_companies_long.csv').resolve()}")

    # Pivotï¼ˆå…ˆç”¨ä»£ç¢¼ï¼‰
    pivot = merged.pivot_table(
        index=["å…¬å¸ä»£ç¢¼", "å…¬å¸åç¨±", "å¹´åº¦"],
        columns="æŒ‡æ¨™ä»£ç¢¼",
        values="æ•¸å€¼",
        aggfunc="first"
    ).reset_index()

    # ä¿ç•™åŸå§‹æ¬„ä½é †åºï¼ˆä¾ merged é¦–æ¬¡å‡ºç¾é †åºï¼‰
    col_order = (
        merged.drop_duplicates(subset=["æŒ‡æ¨™ä»£ç¢¼"])["æŒ‡æ¨™ä»£ç¢¼"].tolist()
    )

    if USE_CHINESE_HEADERS:
        # ä»£ç¢¼ â†’ ä¸­æ–‡æ¬„ä½å å°ç…§
        code_to_cn = (
            merged[["æŒ‡æ¨™ä»£ç¢¼", "æ¬„ä½åç¨±"]]
            .drop_duplicates()
            .set_index("æŒ‡æ¨™ä»£ç¢¼")["æ¬„ä½åç¨±"]
            .to_dict()
        )
        pivot = pivot.rename(columns=code_to_cn)
        col_order = [code_to_cn.get(c, c) for c in col_order]

    # é‡æ–°æ’æ¬„ä½é †åºï¼ˆåªä¿ç•™çœŸçš„å­˜åœ¨çš„æ¬„ä½ï¼Œé¿å… KeyErrorï¼‰
    valid_cols = [c for c in col_order if c in pivot.columns]
    pivot = pivot[["å…¬å¸ä»£ç¢¼", "å…¬å¸åç¨±", "å¹´åº¦"] + valid_cols]

    out_pivot = OUTDIR / "all_companies_pivot.csv"
    pivot.to_csv(out_pivot, index=False, encoding="utf-8-sig")
    print(f"ğŸ“„ Pivot è¼¸å‡ºï¼ˆä¾åŸå§‹é †åºï¼‰â†’ {out_pivot.resolve()}")

    # åˆä½µ raw JSON æˆæ•´åŒ…ï¼ˆçµ¦å‰ç«¯ä¸€æ¬¡æŠ“å…¨éƒ¨ï¼‰
    big_json = {}
    for raw_file in RAW_DIR.glob("*_raw.json"):
        parts = raw_file.stem.split("_")
        code_key, year_key = parts[0], parts[1]
        with open(raw_file, encoding="utf-8") as f:
            big_json.setdefault(code_key, {})[year_key] = json.load(f)
    with open(OUTDIR / "all_companies.json", "w", encoding="utf-8") as f:
        json.dump(big_json, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“„ JSON è¼¸å‡ºï¼š{(OUTDIR / 'all_companies.json').resolve()}")

    # ç”¢ç”ŸæŸ¥è¡¨ç‰ˆ JSONï¼ˆç”¨ä¸­æ–‡ key ç›´æ¥æŸ¥ï¼‰
    lookup_records = make_lookup_records(merged)
    out_lookup = OUTDIR / "all_companies_lookup.json"
    out_lookup.write_text(json.dumps(lookup_records, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"ğŸ“„ Lookup JSON è¼¸å‡ºï¼š{out_lookup.resolve()}")


if __name__ == "__main__":
    main()
